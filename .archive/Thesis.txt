Quantifying Dataset and Model Complexity

In this study, we quantify the complexity of both datasets and models using heuristic formulas inspired by established principles in machine learning and information theory. The dataset complexity (ğ¶ğ·) is approximated based on the size of the dataset and the diversity of its classes. A logarithmic scale is used to capture the diminishing returns of increasing dataset size, combined with a class count factor:

ğ¶ğ·=log2(datasetÂ sizeÂ inÂ MB)+classÂ countÂ factor

The logarithmic component reflects the idea that larger datasets contribute incrementally less new information, a concept aligned with intrinsic dimensionality (Pope et al., 2021). The class count factor adjusts for the variety of labels, acknowledging that more classes generally increase dataset complexity.

The model complexity (ğ¶ğ‘€) is estimated using the number of parameters, a widely accepted proxy for model capacity, combined with a depth factor that accounts for architectural differences (Kaplan et al., 2020). The formula is given as:

ğ¶ğ‘€=log2(parameterÂ countÂ inÂ millions)+depthÂ factor

This approach aligns with scaling laws in neural networks, where model performance often correlates logarithmically with parameter size. The depth factor reflects the observation that deeper models can capture more complex hierarchical representations, thus contributing to overall model complexity.

These formulas provide a practical, adaptable method for quantifying dataset and model complexity, suitable for dynamic environments like deep learning job scheduling, while remaining grounded in established theoretical foundations.

---

